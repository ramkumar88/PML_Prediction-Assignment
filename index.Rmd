Predicting exercise activity 
========================================================
##### Author: Ramkumar Bommireddipalli
##### Published: `r format(Sys.Date(), format="%B %d, %Y")`


Synopsis
========================================================
In this report our goal is to predict the manner in which a person exercises based on the acvitiy data.

To perform the predictions, we obtained the test and training data from the groupware human activity recognition [http://groupware.les.inf.puc-rio.br/har]. The random forest prediction model was selected to get highly accurate predictions.

Initial Setup
========================================================
The caret library is required for the method used to fit a training model and test predictions. The random forest method was selected to fit the model for highly accurate predictions.
```{r setup_libraries}
library(caret)
library(randomForest)
```

Getting Data
========================================================
The  testing and training csv files were obtained from the groupware human activity dataset. The csv files are then loaded into training and testing data frames as show below.
```{r reading_data}
dataDir <- "data"
trainingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainingCSV <- file.path(dataDir,"pml-training.csv")
testingUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testingCSV <- file.path(dataDir,"pml-testing.csv")
if (!file.exists(dataDir)){
    dir.create(file.path(dataDir))
}
## read csv if raw data is not defined
if (!file.exists(trainingCSV)){
    download.file(trainingUrl, destfile = trainingCSV, method="curl")
}
## read csv if raw data is not defined
if (!file.exists(testingCSV)){
    download.file(testingUrl, destfile = testingCSV, method="curl")
}
## Load the data from csv
trainingData <- read.csv(trainingCSV)
testingData <- read.csv(testingCSV)
```


To better validate the trained modal, the training data was split further with 70% for subTraining and 30 % for validation data sets.
```{r setup_training}
# set seed
set.seed(13343)

## Split training to training and validation
inTrain <- createDataPartition(y=trainingData$classe,p=0.70,list=FALSE)
validationData <- trainingData[-inTrain,]
subTrainingData <- trainingData[inTrain,]
```

Cleaning & Pre-processing Data
========================================================
Prior to training a model, it is important to pre-process the training data set to focus on the important variables that have the biggest effect on prediction. The initial training data had over `r orig_var_count <- ncol(subTrainingData)-1;orig_var_count` variables. The first pre-process step is to remove variables variables with NA values and with near zero variance. This will remove variables that do not contribute much to the prediction of the model.
```{r preprocessing_filter_vars}
## Remove all columns with NA
subTrainingData <- subTrainingData[,colSums(is.na(subTrainingData))==0]
## Remove near zero variance data from training
subTrainingData <- subTrainingData[,-nearZeroVar(subTrainingData)]

```

The next step of the analysis is to transform the data to improve the training predictions. The following code looks for the different classes of variables in the training data set.
```{r cleaning_data_types}
variableTypes <- unique(lapply(subTrainingData, class))
variableTypes
```

As we can see, there are some factor class variables in the training set. These need to be transformed to indicator variables to be considerd for training the model. The following code converts the factor variables to indicator variables.
```{r cleaning_data_transform}
## convert factors to indicator variables on training
subTrainingData.dummyvars <- dummyVars(classe ~ ., data=subTrainingData)
subTrainingData.forest <- as.data.frame(predict(subTrainingData.dummyvars, newdata=subTrainingData))
subTrainingData.forest <- cbind(subTrainingData$classe,subTrainingData.forest)
colnames(subTrainingData.forest)[1] <- "classe"
subTrainingData <- subTrainingData.forest

```

After removing the NA, near zero variance variables and transforming the factor variables, there are `r clean_var_count <- ncol(subTrainingData)-1;clean_var_count` variables. This reduced the total variables by about `r round((1 - (clean_var_count/orig_var_count))*100,2)`%.


Feature selection
========================================================
The number of variables can be further reduced to the specific features that have the highest predictive value. This can be achieved by identifying the variables with a high co-variance. In this case a criteria of co-variance of > 0.8 was applied using the cor method.
```{r feature_selection}
## Get the most correlated variables
variableCorrelations <-  abs(cor(subTrainingData[,-1]))
## Clear out the diagnoal
diag(variableCorrelations) <- 0
## Get the variables with high correlations > 0.80
highestCorVarNames<- which(variableCorrelations > 0.80,arr.ind=T)
## Get the unique list of variables
highestCorVarNames <- unique(names(highestCorVarNames[,1]))
```

After selecting the highly co-variate features, there are `r highCorVarLen <- length(highestCorVarNames)-1;highCorVarLen` variables. This is a total reduction of `r round((1 - (highCorVarLen/orig_var_count))*100,2)`% from the original set of variables. The best training variables are then extracted from the sub training data set to train the prediction model.
```{r feature_selection_subset}
## Subset the specific variables from training
bestTrainingVariables <- subTrainingData[ , which(names(subTrainingData) %in% highestCorVarNames)]
```


Training the model with cross validation
========================================================
The best training variables are then to used to train a random forest model with cross validation. The resulting model has close to 100% accuracy as shown in the details below.
```{r training_model,cache=TRUE}
#fit the model
modelFit <- train(subTrainingData$classe~ .,method="rf",data=bestTrainingVariables,trControl=trainControl(method = 'cv'))

## see model details
modelFit
```


Validating the Model
========================================================
The next step is to predict the validation data and verify the accuracy of the model. Since the validation data also has some factor columns, there is some cleanup performed on the validation data.
```{r validate_model}
## Remove all columns with NA
validationData <- validationData[,colSums(is.na(validationData))==0]
## Remove near zero variance data from training
validationData <- validationData[,-nearZeroVar(validationData)]

## convert factors to indicator variables on training
validationData.dummyvars <- dummyVars(classe ~ ., data=validationData)
validationData.forest <- as.data.frame(predict(validationData.dummyvars, newdata=validationData))
validationData.forest <- cbind(validationData$classe,validationData.forest)
colnames(validationData.forest)[1] <- "classe"
validationData <- validationData.forest

## check for accuracy on validation data
validationPred <- predict(modelFit,validationData)

```

The following table and plot show the quality of predictions on the validation data set. The high prediction values along the table diagonal as well has high percentage of true and very low false predictions on the plot indicate that our model is highly accurate.
```{r validate_model_predictions}
## get the list of correct predictions
validationData$rightPred <- (validationPred == validationData$classe)
## Show accuracy on validation data set
table(validationPred,validationData$classe)
## plot the accuracy of predictions along the top two predictors
qplot(pitch_belt,yaw_belt,colour=rightPred,data=validationData,main="validation data predictions")
```

Out of sample error rate
========================================================
Since the subTraining data set was used for training, the validation data set is considered as out of sample. The out of sample error rate is calculated by looking at the validation predictions as shown below.
```{r out_of_sample_error}
## calculate out of sample error rate
outSampleErr <- round((length(which(!validationData$rightPred))/length(validationData$rightPred))*100,digits=2)
```
The out of sample error rate with validation data set was `r outSampleErr`%.


Testing the Model
========================================================
The final step is to predict the testing data using the model trained from before. Since the testing data also has some factor columns, there is some cleanup performed on the testing data.
```{r test_model}
## Remove all columns with NA
testingData <- testingData[,colSums(is.na(testingData))==0]
## Remove near zero variance data from training
testingData <- testingData[,-nearZeroVar(testingData)]

## convert factors to indicator variables on training
testingData.dummyvars <- dummyVars( ~ ., data=testingData)
testingData.forest <- as.data.frame(predict(testingData.dummyvars, newdata=testingData))
testingData <- testingData.forest

## check for accuracy on validation data
testingPred <- predict(modelFit,testingData)

## Show predictions from the testing data set
testingPred
```

The following method is used to generate the files with responses. These files were submitted and yielded 100% correct results.
```{r test_model_results_write}
testResultDir <- "test_results"

## create test results directory
if (!file.exists(testResultDir)){
    dir.create(file.path(testResultDir))
}

## declare method to write each result to a file
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = file.path(testResultDir,paste0("problem_id_",i,".txt"))
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
## call method to write the reults
pml_write_files(testingPred)
```
